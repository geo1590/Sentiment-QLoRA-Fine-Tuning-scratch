{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<div style=\"color:#ffffff;\n          font-size:50px;\n          font-style:italic;\n          text-align:left;\n          font-family: 'Lucida Bright';\n          background:#4686C8;\">\n  \t&nbsp; Sentiment using QLoRA Fine-Tuning from scratch\n</div>\n<br>   \n<div style=\"\n          font-size:20px;\n          text-align:left;\n          font-family: 'Palatino';\n          \">\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project: Sentiment Analysis using QLoRA Fine-Tuning from scratch<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Author: George Barrinuevo<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Date: 07/24/2025<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<br><div style=\"color:#ffffff;\n          font-size:30px;\n          font-style:italic;\n          text-align:left;\n          font-family: 'Lucida Bright';\n          background:#4686C8;\">\n  \t      &nbsp; Project Notes\n</div>\n<div style=\"\n          font-size:16px;\n          text-align:left;\n          font-family: 'Cambria';\">\n    \n**Here are my thoughts on this project**\n- The purpose of this project is to create a Financial Sentiment Analysis using QLoRA, all from scratch . It uses PyTorch and Python code.\n<br>\n\n**Technical Details**\n\n<u>Fine-Tuning</u>\n\n  - This is a technique that uses large pretrained models and weights and then further trains them using domain specific datasets. The result is fast training and specialization on the specific domain.\n\n<u>LoRA</u>\n\n  - Low-Rank Adaptation (LoRA) is a Fine-Tuning method that will save time and money compared with the standard way of training the entire model, while retaining about 90-95% of the performance.\n  - This method will only train some of the parameters. It does this by taking the weights W and dividing them in to B and A matrices, which has less number of parameters than the original W. As a result, the training time is reduced.\n  - The LoraConfig and PeftConfig are used to implement this feature.\n\n<u>PEFT</u>\n  \n  - Parameter-Efficient Fine-Tuning (PEFT) provides a way to use LoRA as the method to update the matrices. Here are some common parameters:\n    * r - The rank of the update matrices\n    * target_modules - The models to apply to the LoRA update matrices. An example is the attention blocks.\n    * lora_alpha - A scaling factor used when updating the weights.\n  - The LoraConfig and PeftConfig are used to implement this feature.\n\n<u>Quantization</u>\n\n  - This is an optimization technique to speed up the training and inference by using lower precision floating point numbers.\n  - Instead of using a 32-bit floats, Quantization uses 16 or 8 or 4 bit floats. This has less precision, but uses less memory and speeds up calculations.\n  - It still uses the same number of trainable parameters.\n  - De-quantization involves taking say the 4-bit float and temporarily converting it to a 8, 16, or 32 bit float used for the calculation part, then stores the results back to 4-bit float. Integers can also be used.\n  - The trade-offs are:\n    * More Precision - This has more accuracy, but at the expense of using more memory and more calculations. The result is slower training and inferences, but more precision.\n    * Less Precision - This is faster and save memory, but at the expense of being less accurate. The result is faster training and inferences, but less precision.\n  - The BitsAndBytesConfig package is used to implement this feature.\n\n<u>Model</u>\n\n  - The pretrained model used is 'google/gemma-2-2b'. This model is already trained. But, we will further train it using a dataset so it can specialize on that dataset.\n  - This function is used to load this pretrained model: AutoModelForCausalLM.from_pretrained().\n\n<u>Tokenizer</u>\n\n  - The Tokenizer to use is the one associated to the same model.\n  - A Tokenizer is used to convert words in to Token IDs and vice versa.\n  - This is the function used to initialize the Tokenizer: AutoTokenizer.from_pretrained().\n\n<u>Dataset</u>\n\n  - A Dataset is needed to further train the pretrained model in order to gain a specialization on a specific task.\n  - In this dataset, there is text consisting of financial information. It also has the sentiment rating.\n  - This dataset can be downloaded here: https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis\n  - This script will auto download this dataset file.\n  - Here is an example row from this dataset:<br>\n        Index: 8<br>\n        Text: 'Kone 's net sales rose by some 14 % year-on-year in the first nine months of 2008.'<br>\n        Sentiment: positive<br>\n\n<u>Dataset Preprocessing</u>\n\n  - The preprocessing stage involves re-formatting the dataset in to a format that the model expects.\n  - Here is a summary of dataset preprocessing this script will do:\n    * Download the Kaggle dataset.\n    * Filter dataset for sentiment that have one of these: positive, neutral, negative.\n    * Shuffle the dataset.\n    * Only the first dataset_size of the dataset is used.\n    * The dataset will be divided between training and testing datasets.\n    * Take the training dataset and divide that in to training and evaluation datasets.\n    * Reformat the datasets to create an input text the model expects. See the sub-section 'Input Text Format' below.\n    * Since the index values will not be consecuted integers because rows where removed, we need to renumber the index of the target or truth dataset. In this script, that would be the 'y_true' variable.\n\n<u>Input Text Format</u>\n\n  - This is for training the model. The dataset is NOT in a format the model expects. This format includes instructions, the financial text, and the sentiment truth. This is the input text passed in to the model so that it can learn from it. The generate_prompt() is used for this. So, here is the basic format for the input text:<br>\n            Analyze the sentiment of the news headline enclosed in square brackets,<br>\n            determine if it is positive, neutral, or negative, and return the answer as<br> \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".<br>\n            \\[Kone 's net sales rose by some 14 % year-on-year in the first nine months of 2008.] = positive<br>\n  - This is for inference/prediction after the Fine-Tuning training is completed. This format is the same as for the training version above except for the sentiment (e.g. positive, neutral, or negative) part is not used. The generate_test_prompt() is used for this.\n\n<u>Auto-Resume & Caching</u>\n\n  - Caching Model & Weights & Dataset\n    * The Model, Weights, Tokenizer, and Dataset are downloaded from the internet. These are saved in to a local cache directory. Once cached, re-running the script will just retrieve this data from the cache saving time. But in Kaggle, if the session is stopped, these caches are deleted. It is recommended to save these cache to a persistent storage.\n  - Checkpoints & Auto-Resume\n    * The purpose of Checkpoints is to save the trainable weights every few number of steps during training so that when the script re-runs, these data is pulled from the cache and can continue training since the last checkpoint.\n    * The auto-resume from the last saved checkpoint is used if the script run is cancelled or crashes.\n\n<u>Multiple GPUs</u>\n\n  - The number of batch sizes (batch_size) must be evenly divisible by the number of GPUs (num_gpu).\n  - If this script detects multple GPUs, then will use custom_trainer_multple_gpu(), else will use trainer.train().\n\n<u>Trainer</u>\n\n  - The type of training method used is to use the TrainingArguments() class where the training parameters are specified and to use the SFTTrainer() class. This differs from creating a custom training loop.\n\n<u>Misc</u>\n\n  - Progress Bar\n    * A progress bar is used (e.g. 35% completed) when downloading data. The tqdm python package is used for this.\n  - API Keys\n    * The HuggingFace API Key 'HF_TOKEN' needs to be added to this Jupyter notebook. This can be created in https://huggingface.co site.\n    * The Weights and Biases API Key 'WANDB_API_KEY' needs to be added to this Jupyter notebook. This can be created in https://wandb.ai/ site.\n</div>","metadata":{}},{"cell_type":"code","source":"# Changable user parameters.\n\nload_model = False         # True: Load the previously saved model and weights.\nsave_model = True          # True: After model has been trained will ave the model and weights.\nnum_epochs = 10            # Default: 10\ndataset_size = 300         # Default: 300\nbatch_size = 2             # Default: 2, must divide evenly with the number of GPUs, e.g. dataset_size % num_gpu = 0.\nsave_every_num_epochs = 5  # Default: 5, save checkpoint every epoch intervals, used only when using multiple GPUs.\n                           # For single GPUs, it will save at every epoch.\n\nbase_path = '/kaggle'     # This /kaggle path will persist between session restarts.\nimport os\nos.environ[\"BASE_PATH\"] = base_path\noutput_trained_weights_dir = base_path + \"/trained_weights\"\nsaved_model_dir = base_path + '/saved_model'\ncache_dir = base_path + '/cache'   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:50:24.006812Z","iopub.execute_input":"2025-07-25T06:50:24.007071Z","iopub.status.idle":"2025-07-25T06:50:24.015778Z","shell.execute_reply.started":"2025-07-25T06:50:24.007048Z","shell.execute_reply":"2025-07-25T06:50:24.015086Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Show the amount of RAM and disk space BEFORE starting this script.\n\ndef show_ram_disk():\n    !free -h\n    print(f'='*70)\n    !df -h $BASE_PATH\n    print(f'='*70)\n    !ls -al $BASE_PATH\n\nshow_ram_disk()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:50:24.022040Z","iopub.execute_input":"2025-07-25T06:50:24.022382Z","iopub.status.idle":"2025-07-25T06:50:24.413547Z","shell.execute_reply.started":"2025-07-25T06:50:24.022366Z","shell.execute_reply":"2025-07-25T06:50:24.412841Z"}},"outputs":[{"name":"stdout","text":"               total        used        free      shared  buff/cache   available\nMem:            31Gi       910Mi        23Gi       2.0Mi       7.4Gi        30Gi\nSwap:             0B          0B          0B\n======================================================================\nFilesystem      Size  Used Avail Use% Mounted on\noverlay         7.9T  6.3T  1.7T  80% /\n======================================================================\ntotal 20\ndrwxr-xr-x 5 root root 4096 Jul 25 06:49 .\ndrwxr-xr-x 1 root root 4096 Jul 25 06:49 ..\ndrwxr-xr-x 3 root root 4096 Jul 25 06:49 input\ndrwxr-xr-x 3 root root 4096 Jul 25 06:49 lib\ndrwxr-xr-x 3 root root 4096 Jul 25 06:49 working\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U datasets\n!pip install -q -U trl\n!pip install -q -U peft\n!pip install -q -U huggingface_hub","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-07-25T06:50:24.415006Z","iopub.execute_input":"2025-07-25T06:50:24.415234Z","iopub.status.idle":"2025-07-25T06:52:16.280876Z","shell.execute_reply.started":"2025-07-25T06:50:24.415210Z","shell.execute_reply":"2025-07-25T06:52:16.280176Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.7/515.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom kaggle_secrets import UserSecretsClient\nimport kagglehub\nimport logging","metadata":{"papermill":{"duration":14.485002,"end_time":"2023-10-16T11:00:18.917449","exception":false,"start_time":"2023-10-16T11:00:04.432447","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:16.281856Z","iopub.execute_input":"2025-07-25T06:52:16.282080Z","iopub.status.idle":"2025-07-25T06:52:45.448094Z","shell.execute_reply.started":"2025-07-25T06:52:16.282056Z","shell.execute_reply":"2025-07-25T06:52:45.447314Z"}},"outputs":[{"name":"stderr","text":"2025-07-25 06:52:29.316848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753426349.539797      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753426349.603334      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nnum_gpu = torch.cuda.device_count()\nprint(f'num_gpu: {num_gpu}')\n\nif batch_size % num_gpu != 0:\n    print(f'The number of GPUs ({num_gpu}) divide evenly with batch_size ({batch_size}), exiting.')\n    exit(1)\n\nif num_gpu == 1:\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nelse:\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:45.449661Z","iopub.execute_input":"2025-07-25T06:52:45.450191Z","iopub.status.idle":"2025-07-25T06:52:45.455042Z","shell.execute_reply.started":"2025-07-25T06:52:45.450170Z","shell.execute_reply":"2025-07-25T06:52:45.454209Z"}},"outputs":[{"name":"stdout","text":"num_gpu: 1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(f\"pytorch version {torch.__version__}\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")\n\nwarnings.filterwarnings(\"ignore\")\n\nuser_secrets = UserSecretsClient()\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:45.455827Z","iopub.execute_input":"2025-07-25T06:52:45.456056Z","iopub.status.idle":"2025-07-25T06:52:45.680043Z","shell.execute_reply.started":"2025-07-25T06:52:45.456039Z","shell.execute_reply":"2025-07-25T06:52:45.679545Z"}},"outputs":[{"name":"stdout","text":"pytorch version 2.6.0+cu124\nworking on cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\ndef evaluate(y_true, y_pred):\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n\n    class_labels = ['negative', 'neutral', 'positive']\n\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    unique_labels = set(y_true) \n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label `{class_labels[label]}`: {accuracy:.3f}')\n\ndef predict(predictor_model, tokenizer):\n    level1 = logging.getLogger(\"transformers\").level\n    logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n    \n    y_pred = []\n    \n    pipe = pipeline(task=\"text-generation\", \n            model=predictor_model, \n            tokenizer=tokenizer, \n            max_new_tokens = 1, \n            temperature = 0.1,\n            use_cache=False,\n    )\n\n    display_every_num_secs = 2\n    for i in tqdm(range(len(X_test)), mininterval=display_every_num_secs):\n        prompt = X_test.iloc[i][\"text\"]\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n\n    logging.getLogger(\"transformers\").setLevel(level1)\n    return y_pred\n\ndef test_prediction(index):\n    pipe = pipeline(task=\"text-generation\", # text-generation\n            model=model, \n            tokenizer=tokenizer, \n            max_new_tokens = 1, \n            temperature = 0.1,\n            use_cache=False,\n    )\n    \n    prompt = X_test.iloc[index][\"text\"]\n    result = pipe(prompt)\n    pred_answer = result[0]['generated_text'].split(\"=\")[-1]\n    true_answer = y_true[index]\n    print(f'prompt: {prompt}')\n    print(f'result: {result}')\n    print(f'pred_answer: {pred_answer}')\n    print(f'true_answer: {true_answer}')\n\ndef save_checkpoint(model, optimizer, epoch, loss, filepath):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(checkpoint, filepath)\n\ndef load_checkpoint(filepath, model, optimizer):\n    if os.path.exists(filepath):\n        checkpoint = torch.load(filepath)\n        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        last_epoch = checkpoint['epoch']\n        loss = checkpoint['loss']\n    else:\n        last_epoch = 0\n\n    return last_epoch\n\ndef custom_trainer_multple_gpu():\n    global train_data\n    global output_trained_weights_dir\n\n    def tokenize_fn(example):\n        tokens = tokenizer(\n            example[\"text\"], \n            padding=\"max_length\", \n            truncation=True, \n            max_length=128\n        )\n        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n        return tokens\n        \n    \n    train_data = train_data.map(tokenize_fn, batched=True)\n    train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n   \n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=None,\n    )\n   \n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f'device: {device}')\n    _ = model.to(device)\n    os.makedirs(output_trained_weights_dir, exist_ok=True)\n\n    checkpoint_file = os.path.join(output_trained_weights_dir, f\"model_weights.pth\")\n    print(f'checkpoint_file: {checkpoint_file}')\n\n    last_epoch = load_checkpoint(checkpoint_file, model, optimizer)       # The first epoch number is '1'.\n    print(f'Last epoch trained: {last_epoch}, total epochs to train: {num_epochs}, saving checkpoint every {save_every_num_epochs} epochs.')\n    last_epoch += 1\n    \n    for epoch in range(last_epoch, num_epochs+1):\n        epoch_loss = 0.0\n        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch}\")):\n            batch = {k: v.to(device) for k, v in batch.items()}\n    \n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=batch[\"labels\"]\n            )\n    \n            loss = outputs.loss    \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            epoch_loss += loss.item()\n\n        if epoch % save_every_num_epochs == 0:\n            save_checkpoint(model, optimizer, epoch, loss.item(), checkpoint_file)\n            print(f\"Saved checkpoint at: {checkpoint_file}\")\n        \n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n        # print(f\"Epoch {epoch} finished - Avg Loss: {avg_epoch_loss:.4f}\")\n\ndef run_trainer():\n    if not load_model:\n        level1 = logging.getLogger(\"transformers\").level\n        # logging.getLogger(\"transformers\").setLevel(logging.INFO)\n        try:\n            if num_gpu > 1:\n                custom_trainer_multple_gpu()\n            else:\n                trainer.train(resume_from_checkpoint=True)\n        except ValueError:\n            if num_gpu == 1:\n                trainer.train()   \n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            print(f'Exception type: {type(e)}')\n            print(f'Exception args: {e.args}')\n            \n        logging.getLogger(\"transformers\").setLevel(level1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:45.680771Z","iopub.execute_input":"2025-07-25T06:52:45.680974Z","iopub.status.idle":"2025-07-25T06:52:45.698270Z","shell.execute_reply.started":"2025-07-25T06:52:45.680957Z","shell.execute_reply":"2025-07-25T06:52:45.697483Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"filename = kagglehub.dataset_download(\"sbhatti/financial-sentiment-analysis\") + '/data.csv'\nprint(\"Filename to dataset files:\", filename)\n\ndf = pd.read_csv(filename, encoding=\"utf-8\", encoding_errors=\"replace\")\ndf.columns = [\"text\", \"sentiment\"]\npd.set_option('display.max_colwidth', 200)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:45.699071Z","iopub.execute_input":"2025-07-25T06:52:45.699303Z","iopub.status.idle":"2025-07-25T06:52:45.968950Z","shell.execute_reply.started":"2025-07-25T06:52:45.699286Z","shell.execute_reply":"2025-07-25T06:52:45.968274Z"}},"outputs":[{"name":"stdout","text":"Filename to dataset files: /kaggle/input/financial-sentiment-analysis/data.csv\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                                                      text  \\\n0  The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and power...   \n1                                                                                                                                                  $ESI on lows, down $1.50 to $2.50 BK a real possibility   \n2        For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .   \n3                                                                         According to the Finnish-Russian Chamber of Commerce , all the major construction companies of Finland are operating in Russia .   \n4                                                                  The Swedish buyout firm has sold its remaining 22.4 percent stake , almost eighteen months after taking the company public in Finland .   \n\n  sentiment  \n0  positive  \n1  negative  \n2  positive  \n3   neutral  \n4   neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and power...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$ESI on lows, down $1.50 to $2.50 BK a real possibility</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>According to the Finnish-Russian Chamber of Commerce , all the major construction companies of Finland are operating in Russia .</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Swedish buyout firm has sold its remaining 22.4 percent stake , almost eighteen months after taking the company public in Finland .</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"sentiment = [\"positive\", \"neutral\", \"negative\"]\ntemp_df = df[df.sentiment.isin(sentiment)]     # Filter\ntemp_df = temp_df.sample(frac=1, random_state=42).reset_index(drop=True)    # Shuffle\ntemp_df = df.head(dataset_size)    # Want only dataset_size rows.\n\nX_train, X_test = train_test_split(\n    temp_df, test_size=0.2, random_state=42, shuffle=True\n)\n\nX_train, X_eval = train_test_split(\n    X_train, test_size=0.25, random_state=42, shuffle=True\n)\n\nX_train = X_train.reset_index(drop=True)\n\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]    # Select indices NOT in X_train or X_test.\nX_eval = df[df.index.isin(eval_idx)]    # Get the dataframe of eval_idx indices.\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)                              # Group the rows be values under the 'sentiment' column.\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))     # Randomly sample 50 rows from each group. If < 50, then duplicate the rows til you have 50 per group.\nX_train = X_train.reset_index(drop=True)    # Renumber the index column in ascending order.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:45.969820Z","iopub.execute_input":"2025-07-25T06:52:45.970597Z","iopub.status.idle":"2025-07-25T06:52:46.069541Z","shell.execute_reply.started":"2025-07-25T06:52:45.970571Z","shell.execute_reply":"2025-07-25T06:52:46.068814Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_name = 'google/gemma-2-2b'\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n    attn_implementation='eager',\n    cache_dir=cache_dir,\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=512)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:52:46.070395Z","iopub.execute_input":"2025-07-25T06:52:46.070651Z","iopub.status.idle":"2025-07-25T06:54:47.839201Z","shell.execute_reply.started":"2025-07-25T06:52:46.070629Z","shell.execute_reply":"2025-07-25T06:54:47.838617Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0125f7f436b1449b838f311a7150823f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cab591b3644aa8993c7bacf7aaf82c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2198773303e94fb7badfc80edd3f66a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b0e8ad09884ffba6c5046deebc09e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31ecaa8d75d49bb9f3f01cba7eca682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af021f789ea4463a8973bebd0226750"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aff09fc188c4e318f0f6e2d371662cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade6f4d296e44b4b81e3f60027b2adba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cc90a52f50e4cb6905c0a0c37ce76c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec365e1a8f354884a0d9d56d1e958a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c222451b324423b8f289fe75e0e8849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f626a5fff842cd9fc69f54eba244c0"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), columns=[\"text\"])\n\ny_true = X_test.sentiment\ny_true = y_true.reset_index(drop=True)    # Renumber the index column.\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:47.841234Z","iopub.execute_input":"2025-07-25T06:54:47.841497Z","iopub.status.idle":"2025-07-25T06:54:47.868579Z","shell.execute_reply.started":"2025-07-25T06:54:47.841479Z","shell.execute_reply":"2025-07-25T06:54:47.868007Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Predict before the Fine-Tuning.\ny_pred = predict(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:47.869168Z","iopub.execute_input":"2025-07-25T06:54:47.869334Z","iopub.status.idle":"2025-07-25T06:54:55.798558Z","shell.execute_reply.started":"2025-07-25T06:54:47.869321Z","shell.execute_reply":"2025-07-25T06:54:55.797679Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 60/60 [00:07<00:00,  7.57it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Evaluate before the Fine-Tuning\nevaluate(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:55.799405Z","iopub.execute_input":"2025-07-25T06:54:55.799725Z","iopub.status.idle":"2025-07-25T06:54:55.808585Z","shell.execute_reply.started":"2025-07-25T06:54:55.799690Z","shell.execute_reply":"2025-07-25T06:54:55.807915Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.417\nAccuracy for label `negative`: 0.250\nAccuracy for label `neutral`: 0.618\nAccuracy for label `positive`: 0.071\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],   # For 'google/gemma-2-2b' model.\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_trained_weights_dir,    # Where checkpoints and weights will be saved.\n    num_train_epochs=num_epochs,     \n    per_device_train_batch_size=batch_size,   # Batch-size. The SFTTrainer() will build a dataloader internally.\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=25, \n    learning_rate=1e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n    eval_strategy=\"epoch\",                   # For checkpoint, must match save_strategy=\n    save_strategy=\"epoch\",                   # Save checkpoints at each epoch.\n    save_total_limit=3,                      # Keep only last 3 checkpoints.\n    # save_steps=20,                           # Save checkpoint after every save_steps= batch.\n    load_best_model_at_end=True,             # Optional: loads best checkpoint after training\n    resume_from_checkpoint=True,             # This triggers checkpoint auto-resume.\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:55.809474Z","iopub.execute_input":"2025-07-25T06:54:55.809980Z","iopub.status.idle":"2025-07-25T06:54:59.403862Z","shell.execute_reply.started":"2025-07-25T06:54:55.809957Z","shell.execute_reply":"2025-07-25T06:54:59.403262Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d087a3d9b694e88b72fd2b2c53a62cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b16f1997cd444392da94085829b170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/180 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16115cd8cbc4b8fb739670315c4306e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92554d8faafa4b079f5093eddb336210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5a28240e394b14b2477a5dc45f0e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ff214ee7784eaa8a80542a28104294"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(f'load_model: {load_model}')\nprint(f'save_model: {save_model}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:59.404536Z","iopub.execute_input":"2025-07-25T06:54:59.404718Z","iopub.status.idle":"2025-07-25T06:54:59.408910Z","shell.execute_reply.started":"2025-07-25T06:54:59.404702Z","shell.execute_reply":"2025-07-25T06:54:59.408134Z"}},"outputs":[{"name":"stdout","text":"load_model: False\nsave_model: True\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"if load_model:\n    model.load_state_dict(torch.load(saved_model_dir))\n    model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:59.409704Z","iopub.execute_input":"2025-07-25T06:54:59.409987Z","iopub.status.idle":"2025-07-25T06:54:59.428380Z","shell.execute_reply.started":"2025-07-25T06:54:59.409959Z","shell.execute_reply":"2025-07-25T06:54:59.427789Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"%%time\n\nprint(f'num_gpu: {num_gpu}')\nrun_trainer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:54:59.429116Z","iopub.execute_input":"2025-07-25T06:54:59.429351Z","iopub.status.idle":"2025-07-25T07:09:58.308966Z","shell.execute_reply.started":"2025-07-25T06:54:59.429334Z","shell.execute_reply":"2025-07-25T07:09:58.308270Z"}},"outputs":[{"name":"stdout","text":"num_gpu: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 14:51, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.470550</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.211083</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.840900</td>\n      <td>1.031574</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.840900</td>\n      <td>1.032261</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.926200</td>\n      <td>1.080405</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.926200</td>\n      <td>1.123204</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.694800</td>\n      <td>1.230017</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.694800</td>\n      <td>1.286817</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.502000</td>\n      <td>1.303989</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.502000</td>\n      <td>1.308968</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec73d82f0494420bf2e3e26a03b7fe7"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 12min 42s, sys: 2min 14s, total: 14min 57s\nWall time: 14min 58s\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"if save_model:\n    !pwd\n    trainer.save_model(saved_model_dir)\n    tokenizer.save_pretrained(saved_model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:09:58.309859Z","iopub.execute_input":"2025-07-25T07:09:58.310146Z","iopub.status.idle":"2025-07-25T07:10:00.499247Z","shell.execute_reply.started":"2025-07-25T07:09:58.310122Z","shell.execute_reply":"2025-07-25T07:10:00.498231Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Predict after the Fine-Tuning.\ny_trained_pred = predict(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:10:00.503593Z","iopub.execute_input":"2025-07-25T07:10:00.503868Z","iopub.status.idle":"2025-07-25T07:10:09.394386Z","shell.execute_reply.started":"2025-07-25T07:10:00.503841Z","shell.execute_reply":"2025-07-25T07:10:09.393641Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 60/60 [00:08<00:00,  6.76it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Evaluate after the Fine-Tuning\nevaluate(y_true, y_trained_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:10:09.395287Z","iopub.execute_input":"2025-07-25T07:10:09.395538Z","iopub.status.idle":"2025-07-25T07:10:09.403467Z","shell.execute_reply.started":"2025-07-25T07:10:09.395521Z","shell.execute_reply":"2025-07-25T07:10:09.402658Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.717\nAccuracy for label `negative`: 0.667\nAccuracy for label `neutral`: 0.735\nAccuracy for label `positive`: 0.714\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"test_prediction(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:10:09.404210Z","iopub.execute_input":"2025-07-25T07:10:09.404546Z","iopub.status.idle":"2025-07-25T07:10:13.689373Z","shell.execute_reply.started":"2025-07-25T07:10:09.404519Z","shell.execute_reply":"2025-07-25T07:10:13.688738Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nThe following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"prompt: Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [$LULU #Outlook #Q1 2013 #lost revenue in the range of $12 million to $17 million -not good] =\nresult: [{'generated_text': 'Analyze the sentiment of the news headline enclosed in square brackets, \\n            determine if it is positive, neutral, or negative, and return the answer as \\n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\\n\\n            [$LULU #Outlook #Q1 2013 #lost revenue in the range of $12 million to $17 million -not good] = negative'}]\npred_answer:  negative\ntrue_answer: negative\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"show_ram_disk()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:10:13.690166Z","iopub.execute_input":"2025-07-25T07:10:13.690398Z","iopub.status.idle":"2025-07-25T07:10:14.354457Z","shell.execute_reply.started":"2025-07-25T07:10:13.690370Z","shell.execute_reply":"2025-07-25T07:10:14.353643Z"}},"outputs":[{"name":"stdout","text":"               total        used        free      shared  buff/cache   available\nMem:            31Gi       4.2Gi       413Mi        17Mi        26Gi        26Gi\nSwap:             0B          0B          0B\n======================================================================\nFilesystem      Size  Used Avail Use% Mounted on\noverlay         7.9T  6.3T  1.6T  80% /\n======================================================================\ntotal 32\ndrwxr-xr-x 8 root root 4096 Jul 25 07:09 .\ndrwxr-xr-x 1 root root 4096 Jul 25 06:49 ..\ndrwxr-xr-x 4 root root 4096 Jul 25 06:52 cache\ndrwxr-xr-x 3 root root 4096 Jul 25 06:49 input\ndrwxr-xr-x 3 root root 4096 Jul 25 06:49 lib\ndrwxr-xr-x 2 root root 4096 Jul 25 07:09 saved_model\ndrwxr-xr-x 6 root root 4096 Jul 25 07:09 trained_weights\ndrwxr-xr-x 3 root root 4096 Jul 25 06:49 working\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### ","metadata":{}}]}